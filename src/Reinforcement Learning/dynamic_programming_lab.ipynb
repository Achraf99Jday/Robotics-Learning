{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dynamic_programming_lab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvLBhVirapw-"
      },
      "source": [
        "# Outlook #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDnFYaehas2B"
      },
      "source": [
        "In this colab we will investigate the **value iteration** and **policy iteration** algorithms in a maze environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILQS3CjXDRO0"
      },
      "source": [
        "# Installation #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU5THTsX5Fmw"
      },
      "source": [
        "!apt-get install ffmpeg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6bDf5E_Hh-3"
      },
      "source": [
        "!pip install git+https://github.com/osigaud/SimpleMazeMDP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PE58RHs9dEEh"
      },
      "source": [
        "import os\n",
        "from typing import Tuple, List\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from mazemdp.maze_plotter import show_videos\n",
        "from mazemdp.mdp import Mdp\n",
        "\n",
        "# For visualization\n",
        "os.environ[\"VIDEO_FPS\"] = \"5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa3oM2C1CqLl"
      },
      "source": [
        "# Agents and MDPs #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eL3B_dpu-H-f"
      },
      "source": [
        "\n",
        "A reinforcement learning agent interacts with an environment represented as a Markov Decision Process (MDP). It is defined by a tuple $(S, A, P, r, \\gamma)$ where $S$ is the state space, $A$ is the action space, $P(state_t, action_t, state_{t+1})$ is the transition function, $r(state_t, action_t)$ is the reward function and $\\gamma \\in [0, 1]$ is the discount factor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-5EU9BalMQI"
      },
      "source": [
        "In what follows we import code to create an MDP corresponding to a random maze (see https://github.com/osigaud/SimpleMazeMDP for documentation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvSGrPnKlXxf"
      },
      "source": [
        "from mazemdp import create_random_maze\n",
        "\n",
        "mdp = create_random_maze(10, 10, 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pL-L0w1ECuah"
      },
      "source": [
        "# Dynamic programming #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHUlXv9TCha4"
      },
      "source": [
        "\n",
        "The goal of an RL agent is to find the optimal behaviour, defined by a policy $\\pi$ that assigns an action (or distribution over actions) to each state so as to maximize the agent's total expected reward. In order to estimate how good a state is, either a state value function $V(x)$ or a state-action value function $Q(x,u)$ is used.\n",
        "\n",
        "Dynamic programming algorithms are used for planning, they require a full knowledge of the MDP from the agent (in contrast to \"true\" RL where the agent does not know the transition and reward functions).\n",
        "They find the optimal policy by computing a value function $V$ or an action-value function $Q$ over the state space or state-action space of the given MDP. **Value iteration** and **policy iteration** are two standard dynamic programming algorithms. You should study both of them using both $V$ and $Q$, as these algorithms contain the basic building blocks for most RL algorithms.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTW9XJD8DjsD"
      },
      "source": [
        "## Value Iteration ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpjrIBqEDmyl"
      },
      "source": [
        "### Value Iteration with the V function ###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eSkbueDDrw2"
      },
      "source": [
        "\n",
        "When using the $V$ function, **value iteration** aims at finding the optimal values $V^*$ based on the Bellman Optimality Equation:\n",
        "$$V^*(s) = \\max_a \\big[ r(s,a) + \\gamma \\sum_{y \\in S} P(s,a,y)V^*(y) \\big],$$\n",
        "\n",
        "where:\n",
        "\n",
        "*   $r(s, a)$ is the reward obtained from taking action $a$ in state $s$,\n",
        "*   $P(s, a, y)$ is the probability of reaching state $y$ when taking action $a$ in state $s$, \n",
        "*   $\\gamma \\in [0,1]$ is a discount factor defining the relative importance of long term rewards over short term ones (the closer to 0, the more the agent focuses on immediate rewards).\n",
        "\n",
        "In practice, we start with an initial value function $V^0$ (for instance, the values of all states are 0), and then we iterate for all states $s$\n",
        "\n",
        "$$V^{i+1}(s) = \\max_a \\big[ r(s,a) + \\gamma \\sum_{y \\in S} P(s,a,y)V^i(y) \\big],$$\n",
        "\n",
        "until the values converge, that is $\\forall s, V^{i+1}(s) \\approx V^i(s)$. It is shown that at convergence, $\\forall s, V^i(s)= V^*(s)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cwhDCVznRfx"
      },
      "source": [
        "To visualize the policy obtained from **value iteration**, we need to first define the `get_policy_from_V()` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLJ6ccP0aNpR"
      },
      "source": [
        "def get_policy_from_v(mdp: Mdp, v: np.ndarray) -> np.ndarray:\n",
        "    # Outputs a policy given the state values\n",
        "    policy = np.zeros(mdp.nb_states)  # initial state values are set to 0\n",
        "    for x in range(mdp.nb_states):  # for each state x\n",
        "        # Compute the value of the state x for each action u of the MDP action space\n",
        "        v_temp = []\n",
        "        for u in mdp.action_space.actions:\n",
        "            if x not in mdp.terminal_states:\n",
        "                # Process sum of the values of the neighbouring states\n",
        "                summ = 0\n",
        "                for y in range(mdp.nb_states):\n",
        "                    summ = summ + mdp.P[x, u, y] * v[y]\n",
        "                v_temp.append(mdp.r[x, u] + mdp.gamma * summ)\n",
        "            else:  # if the state is final, then we only take the reward into account\n",
        "                v_temp.append(mdp.r[x, u])\n",
        "        policy[x] = np.argmax(v_temp)\n",
        "    return policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pX2cGPT9nNga"
      },
      "source": [
        "\n",
        "The `value_iteration_v(mdp)` function below provides the code of **value iteration** using the $V$ function. It is given as an example from which you can derive other instances of dynamic programming algorithms. Look at it more closely, this will help for later questions:\n",
        "\n",
        "* you can ignore the `mdp.new_render()` and `mdp.render(...)` functions which are here to provide the visualization of the iterations.\n",
        "* find in the code the loop over states, the main loop that performs these updates until the values don't change significantly anymore, the main update equation. Found them? OK, you can continue...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUurUq7val7K"
      },
      "source": [
        "# ------------------------- Value Iteration with the V function ----------------------------#\n",
        "# Given a MDP, this algorithm computes the optimal state value function V\n",
        "# It then derives the optimal policy based on this function\n",
        "# This function is given\n",
        "\n",
        "\n",
        "def value_iteration_v(mdp: Mdp, render: bool = True) -> Tuple[np.ndarray, List[float]]:\n",
        "    # Value Iteration using the state value v\n",
        "    v = np.zeros(mdp.nb_states)  # initial state values are set to 0\n",
        "    v_list = []\n",
        "    stop = False\n",
        "\n",
        "    if render:\n",
        "        mdp.new_render(\"Value iteration V\")\n",
        "\n",
        "    while not stop:\n",
        "        v_old = v.copy()\n",
        "        if render:\n",
        "            mdp.render(v, title=\"Value iteration V\")\n",
        "\n",
        "        for x in range(mdp.nb_states):  # for each state x\n",
        "            # Compute the value of the state x for each action u of the MDP action space\n",
        "            v_temp = []\n",
        "            for u in mdp.action_space.actions:\n",
        "                if x not in mdp.terminal_states:\n",
        "                    # Process sum of the values of the neighbouring states\n",
        "                    summ = 0\n",
        "                    for y in range(mdp.nb_states):\n",
        "                        summ = summ + mdp.P[x, u, y] * v_old[y]\n",
        "                    v_temp.append(mdp.r[x, u] + mdp.gamma * summ)\n",
        "                else:  # if the state is final, then we only take the reward into account\n",
        "                    v_temp.append(mdp.r[x, u])\n",
        "\n",
        "                    # Select the highest state value among those computed\n",
        "            v[x] = np.max(v_temp)\n",
        "\n",
        "        # Test if convergence has been reached\n",
        "        if (np.linalg.norm(v - v_old)) < 0.01:\n",
        "            stop = True\n",
        "        v_list.append(np.linalg.norm(v))\n",
        "\n",
        "    if render:\n",
        "        policy = get_policy_from_v(mdp, v)\n",
        "        mdp.render(v, policy, title=\"Value iteration V\")\n",
        "\n",
        "    return v, v_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9eR8ClAmsGH"
      },
      "source": [
        "Let us run it on the previously defined MDP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PTk3t-tmoE4"
      },
      "source": [
        "v, v_list = value_iteration_v(mdp, render=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQI6BSI99CxZ"
      },
      "source": [
        "show_videos(\"videos/\", prefix=\"ValueiterationV\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdWqPhuHE8cd"
      },
      "source": [
        "### Value iteration with the $Q$ function ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K09BVcjDFTSZ"
      },
      "source": [
        "\n",
        "The state-action value function $Q^{\\pi}(s,a)$ defines the value of being in state $s$, taking action $a$ then following policy $\\pi$. The Bellman Optimality Equation for $Q^*$ is\n",
        "$$ Q^*(s,a) =  r(s,a) + \\gamma \\sum_{y} P(s,a,y) \\max_{a'}Q^*(y,a'). $$ \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga2GQ666FZLv"
      },
      "source": [
        "**Question:** By taking inspiration from the `value_iteration_v(mdp)` function above, fill the blank (given with '\\#Q[x,u]=...') in the code of `value_iteration_q(mdp)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnL0la4Vatk0"
      },
      "source": [
        "\n",
        "# ------------------------- Value Iteration with the Q function ----------------------------#\n",
        "# Given a MDP, this algorithm computes the optimal action value function Q\n",
        "# It then derives the optimal policy based on this function\n",
        "\n",
        "\n",
        "def value_iteration_q(mdp: Mdp, render: bool = True) -> Tuple[np.ndarray, List[float]]:\n",
        "    q = np.zeros((mdp.nb_states, mdp.action_space.size))  # initial action values are set to 0\n",
        "    q_list = []\n",
        "    stop = False\n",
        "\n",
        "    if render:\n",
        "        mdp.new_render(\"Value iteration Q\")\n",
        "\n",
        "    while not stop:\n",
        "        qold = q.copy()\n",
        "\n",
        "        if render:\n",
        "            mdp.render(q, title=\"Value iteration Q\")\n",
        "\n",
        "        for x in range(mdp.nb_states):\n",
        "            for u in mdp.action_space.actions:\n",
        "                if x in mdp.terminal_states:\n",
        "                    # q[x, :] = ...\n",
        "                else:\n",
        "                    summ = 0\n",
        "                    for y in range(mdp.nb_states):\n",
        "                        # summ += ...\n",
        "                    # q[x, u] = ...\n",
        "\n",
        "        if (np.linalg.norm(q - qold)) <= 0.01:\n",
        "            stop = True\n",
        "        q_list.append(np.linalg.norm(q))\n",
        "\n",
        "    if render:\n",
        "        mdp.render(q, title=\"Value iteration Q\")\n",
        "    return q, q_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RrcM7TkoPos"
      },
      "source": [
        "Once you are done, run it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVLEEwPwoUee"
      },
      "source": [
        "q, q_list = value_iteration_q(mdp, render=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ngkVo-y9gCG"
      },
      "source": [
        "show_videos(\"videos/\", prefix=\"ValueiterationQ\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN6zdsd9F_dn"
      },
      "source": [
        "## Policy Iteration ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R1x-PDzGH_5"
      },
      "source": [
        "\n",
        "The **policy iteration** algorithm is more complicated than **value iteration**.\n",
        "Given a MDP and a policy $\\pi$, **policy iteration** iterates the following steps: \n",
        "\n",
        "*   Evaluate policy $\\pi$: compute $V$ or $Q$ based on the policy $\\pi$;\n",
        "*   Improve policy $\\pi$: compute a better policy based on $V$ or $Q$.\n",
        "\n",
        "This process is repeated until convergence, i.e. when the policy cannot be improved anymore.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiiWPQO2GlKy"
      },
      "source": [
        "### Policy iteration with the $V$ function ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L0IiPkNGryB"
      },
      "source": [
        "When using $V$, $V^{\\pi}(s)$ is the expected return when starting from state $s$ and following policy $\\pi$. It is processed based on the Bellman Optimality Equation for deterministic policies:\n",
        "\n",
        "$$V^\\pi(s) = r(s, \\pi(s)) + \\gamma \\sum_{y \\in S}P(s, \\pi(s), y)V^\\pi(y),$$\n",
        "\n",
        "\n",
        "where:\n",
        "\n",
        "*   $\\pi$ is a deterministic policy, meaning that in a state $s$, the agent always selects the same action,\n",
        "*   $V^\\pi(y)$ is the value of the state $y$ under policy $\\pi$.\n",
        "\n",
        "\n",
        "Thus, given a policy $\\pi$, one must first compute its value function $V^\\pi(s)$ for all states $s$ iterating the Bellman Optimality Equation until convergence, that is using **value iteration**.\n",
        "Then, one must determine if policy $\\pi$ can be improved based on $V$. For that, in each state $s$, one can compute the Q-value $Q(s,a)$ of applying action $a$ and then following policy $\\pi$ based on the just computed $V^\\pi$, and replace the action $\\pi(s)$ with $\\arg\\max_a Q(s,a)$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ifSjTGpJsBd"
      },
      "source": [
        "\n",
        "In order to facilitate the coding of **policy iteration** algorithms, we first define a set of useful functions.\n",
        "\n",
        "The `improve_policy_from_v(mdp, v, policy)` function is very similar to the `get_policy_from_v(v)` function which was given above. The main difference is that it takes a policy as argument and improves this policy when possible, thus is is more in the spirit of the `policy improvement` step of **policy iteration**. But both functions can be used interchangeably.\n",
        "\n",
        "The functions `evaluate_one_step_v(mdp, v, policy)`, where `mdp` is a given MDP, `v` is some value function in this MDP and `policy` is some policy and the function `evaluate_v(mdp, policy)` are also given. These functions are used to build the value function $V^\\pi$ corresponding to policy $\\pi$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTHPccOTaTMM"
      },
      "source": [
        "def improve_policy_from_v(mdp: Mdp, v: np.ndarray, policy: np.ndarray) -> np.ndarray:\n",
        "    # Improves a policy given the state values\n",
        "    for x in range(mdp.nb_states):  # for each state x\n",
        "        # Compute the value of the state x for each action u of the MDP action space\n",
        "        v_temp = np.zeros(mdp.action_space.size)\n",
        "        for u in mdp.action_space.actions:\n",
        "            if x not in mdp.terminal_states:\n",
        "                # Process sum of the values of the neighbouring states\n",
        "                summ = 0\n",
        "                for y in range(mdp.nb_states):\n",
        "                    summ = summ + mdp.P[x, u, y] * v[y]\n",
        "                v_temp[u] = mdp.r[x, u] + mdp.gamma * summ\n",
        "            else:  # if the state is final, then we only take the reward into account\n",
        "                v_temp[u] = mdp.r[x, u]\n",
        "\n",
        "        for u in mdp.action_space.actions:\n",
        "            if v_temp[u] > v_temp[policy[x]]:\n",
        "                policy[x] = u\n",
        "    return policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTFXjB2laYEt"
      },
      "source": [
        "def evaluate_one_step_v(mdp: Mdp, v: np.ndarray, policy: np.ndarray) -> np.ndarray:\n",
        "    # Outputs the state value function after one step of policy evaluation\n",
        "    # Corresponds to one application of the Bellman Operator\n",
        "    v_new = np.zeros(mdp.nb_states)  # initial state values are set to 0\n",
        "    for x in range(mdp.nb_states):  # for each state x\n",
        "        # Compute the value of the state x for each action u of the MDP action space\n",
        "        if x not in mdp.terminal_states:\n",
        "            # Process sum of the values of the neighbouring states\n",
        "            summ = 0\n",
        "            for y in range(mdp.nb_states):\n",
        "                summ = summ + mdp.P[x, policy[x], y] * v[y]\n",
        "            v_new[x] = mdp.r[x, policy[x]] + mdp.gamma * summ\n",
        "        else:  # if the state is final, then we only take the reward into account\n",
        "            v_new[x] = mdp.r[x, policy[x]]\n",
        "    return v_new\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gb9BGM1aa7p"
      },
      "source": [
        "def evaluate_v(mdp: Mdp, policy: np.ndarray) -> np.ndarray:\n",
        "    # Outputs the state value function of a policy\n",
        "    v = np.zeros(mdp.nb_states)  # initial state values are set to 0\n",
        "    stop = False\n",
        "    while not stop:\n",
        "        vold = v.copy()\n",
        "        v = evaluate_one_step_v(mdp, vold, policy)\n",
        "\n",
        "        # Test if convergence has been reached\n",
        "        if (np.linalg.norm(v - vold)) < 0.01:\n",
        "            stop = True\n",
        "    return v\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpvHat5dqAvl"
      },
      "source": [
        "To perform **policy iteration** we also need an initial random policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2E3k7-vdV81"
      },
      "source": [
        "from mazemdp import random_policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6gyHWWoMF4i"
      },
      "source": [
        "**Question:** By using the above functions, fill the code of the `policy_iteration_v(mdp)` function below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR3FHBGZa5qp"
      },
      "source": [
        "# ------------------------- Policy Iteration with the V function ----------------------------#\n",
        "# Given a MDP, this algorithm simultaneously computes the optimal state value function V and the optimal policy\n",
        "\n",
        "def policy_iteration_v(mdp: Mdp, render: bool = True) -> Tuple[np.ndarray, List[float]]:\n",
        "    # policy iteration over the v function\n",
        "    v = np.zeros(mdp.nb_states)  # initial state values are set to 0\n",
        "    v_list = []\n",
        "    policy = random_policy(mdp)\n",
        "\n",
        "    stop = False\n",
        "\n",
        "    if render:\n",
        "        mdp.new_render(\"Policy iteration V\")\n",
        "\n",
        "    while not stop:\n",
        "        vold = v.copy()\n",
        "        # Step 1 : Policy Evaluation\n",
        "        # v = ...\n",
        "\n",
        "        if render:\n",
        "            mdp.render(v, title=\"Policy iteration V\")\n",
        "            mdp.plotter.render_pi(policy)\n",
        "\n",
        "        # Step 2 : Policy Improvement\n",
        "        # policy = ...\n",
        "\n",
        "        # Check convergence\n",
        "        if (np.linalg.norm(v - vold)) < 0.01:\n",
        "            stop = True\n",
        "        v_list.append(np.linalg.norm(v))\n",
        "\n",
        "    if render:\n",
        "        mdp.render(v, title=\"Policy iteration V\")\n",
        "        mdp.plotter.render_pi(policy)\n",
        "    return v, v_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNCykfE9qKlH"
      },
      "source": [
        "And finally run it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxzgX6UFqOcX"
      },
      "source": [
        "v, v_list = policy_iteration_v(mdp, render=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iV8QlyRz9u8F"
      },
      "source": [
        "show_videos(\"videos/\", prefix=\"PolicyiterationV\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f64zjARKIT7s"
      },
      "source": [
        "### Policy iteration with the $Q$ function ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfOgv6znIbF6"
      },
      "source": [
        "\n",
        "The **policy iteration** algorithm with the $Q$ function is the same as with the $V$ function, but the policy improvement step is more straightforward.\n",
        "\n",
        "When using $Q$, the Bellman Optimality Equation with deterministic policy $\\pi$ for $Q$ becomes: \n",
        "$$Q^{\\pi}(s,a) = r(s,a) + \\gamma \\sum_{y \\in S}P(s,a,y)Q^{\\pi}(y,\\pi(y)).$$\n",
        "\n",
        "The policy can then be updated as follows:\n",
        "$$\\pi^{(t+1)}(s) = \\arg\\max_aQ^{\\pi^{(t)}}(s,a).$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_S-sF4ynqlUF"
      },
      "source": [
        "First, we need to determine a policy from the $Q$ function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDeb82mSJ2gJ"
      },
      "source": [
        "**Question:**  fill the `get_policy_from_q(q)` function, where $q$ is the state-action value function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfNl_xDlSYIy"
      },
      "source": [
        "def get_policy_from_q(q: np.ndarray) -> np.ndarray:\n",
        "    # Outputs a policy given the action values\n",
        "    # ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F_ueGD5K4jI"
      },
      "source": [
        "**Question:** By drawing inspiration on the functions give with the $v$ function, fill the code of the `evaluate_one_step_q(mdp, q, policy)` function below, where $q$ is some action value function, and the `evaluate_q(mdp, policy)` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zSOoqvJafLo"
      },
      "source": [
        "def evaluate_one_step_q(mdp: Mdp, q: np.ndarray, policy: np.ndarray) -> np.ndarray:\n",
        "    # Outputs the state value function after one step of policy evaluation\n",
        "    qnew = np.zeros((mdp.nb_states, mdp.action_space.size))  # initial action values are set to 0\n",
        "    for x in range(mdp.nb_states):  # for each state x\n",
        "        # Compute the value of the state x for each action u of the MDP action space\n",
        "        for u in mdp.action_space.actions:\n",
        "            if x not in mdp.terminal_states:\n",
        "                # Process sum of the values of the neighbouring states\n",
        "                # ...\n",
        "            else:  # if the state is final, then we only take the reward into account\n",
        "                # qnew[x, u] = ...\n",
        "    return qnew"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phlt5QfbaiWw"
      },
      "source": [
        "def evaluate_q(mdp: Mdp, policy: np.ndarray) -> np.ndarray:\n",
        "    # Outputs the state value function of a policy\n",
        "    q = np.zeros((mdp.nb_states, mdp.action_space.size))  # initial action values are set to 0\n",
        "    stop = False\n",
        "    while not stop:\n",
        "        qold = q.copy()\n",
        "        # q = ...\n",
        "\n",
        "        # Test if convergence has been reached\n",
        "        if (np.linalg.norm(q - qold)) < 0.01:\n",
        "            stop = True\n",
        "    return q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO99FFnqL51O"
      },
      "source": [
        "**Question:** By using the above functions, fill the code of the `policy_iteration_q(mdp)` function below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM5vOv5XazEq"
      },
      "source": [
        "# ------------------------- Policy Iteration with the Q function ----------------------------#\n",
        "# Given a MDP, this algorithm simultaneously computes the optimal action value function Q and the optimal policy\n",
        "\n",
        "def policy_iteration_q(mdp: Mdp, render: bool = True) -> Tuple[np.ndarray, List[float]]:\n",
        "    \"\"\"policy iteration over the q function.\"\"\"\n",
        "    q = np.zeros((mdp.nb_states, mdp.action_space.size))  # initial action values are set to 0\n",
        "    q_list = []\n",
        "    policy = random_policy(mdp)\n",
        "\n",
        "    stop = False\n",
        "\n",
        "    if render:\n",
        "        mdp.new_render(\"Policy iteration Q\")\n",
        "\n",
        "    while not stop:\n",
        "        qold = q.copy()\n",
        "\n",
        "        if render:\n",
        "            mdp.render(q, title=\"Policy iteration Q\")\n",
        "\n",
        "        # Step 1 : Policy evaluation\n",
        "        # q = ...\n",
        "\n",
        "        # Step 2 : Policy improvement\n",
        "        # policy = ...\n",
        "\n",
        "        # Check convergence\n",
        "        if (np.linalg.norm(q - qold)) <= 0.01:\n",
        "            stop = True\n",
        "        q_list.append(np.linalg.norm(q))\n",
        "\n",
        "    if render:\n",
        "        mdp.render(q, get_policy_from_q(q), title=\"Policy iteration Q\")\n",
        "    return q, q_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR999T8Hq1c8"
      },
      "source": [
        "Finally, run it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ9YfuNEq43H"
      },
      "source": [
        "q, q_list = policy_iteration_q(mdp, render=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59Vve3vg9zR0"
      },
      "source": [
        "show_videos(\"videos/\", prefix=\"PolicyiterationQ\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edD-FVlaLvnK"
      },
      "source": [
        "### Study part: Experimental comparisons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSnI34THM4Af"
      },
      "source": [
        "We will now compare the efficiency of the various dynamic programming methods using either the $V$ or the  $Q$ functions.\n",
        "\n",
        "In all your dymanic programming functions, add code to count the number of iterations and the number of elementary $V$ or $Q$ updates. Use the provided `mazemdp.Chrono` class to measure the time taken. You may generate various mazes of various sizes to figure out the influence of the maze topology.\n",
        "\n",
        "Build a table where you compare the various dymanic programming functions in terms of iterations, elementary operations and time taken.\n",
        "\n",
        "You can run the `plot_convergence_vi_pi(...)` function provided below to visualize the convergence of the various algorithms.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkfRPpT6a7D6"
      },
      "source": [
        "\n",
        "# -------- plot learning curves of Q-Learning and Sarsa using epsilon-greedy and softmax ----------#\n",
        "\n",
        "\n",
        "def plot_convergence_vi_pi(m, render):\n",
        "    v, v_list1 = value_iteration_v(m, render)\n",
        "    q, q_list1 = value_iteration_q(m, render)\n",
        "    v, v_list2 = policy_iteration_v(m, render)\n",
        "    q, q_list2 = policy_iteration_q(m, render)\n",
        "\n",
        "    plt.plot(range(len(v_list1)), v_list1, label='value_iteration_v')\n",
        "    plt.plot(range(len(q_list1)), q_list1, label='value_iteration_q')\n",
        "    plt.plot(range(len(v_list2)), v_list2, label='policy_iteration_v')\n",
        "    plt.plot(range(len(q_list2)), q_list2, label='policy_iteration_q')\n",
        "\n",
        "    plt.xlabel('Number of episodes')\n",
        "    plt.ylabel('Norm of V or Q value')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.savefig(\"comparison_DP.png\")\n",
        "    plt.title(\"Comparison of convergence rates\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTOUNa1IMb6G"
      },
      "source": [
        "**Question:** Run the code below and visualize the results of the different algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBrTUnLMopHk"
      },
      "source": [
        "plot_convergence_vi_pi(mdp, False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9LttlIXsB9o"
      },
      "source": [
        "\n",
        "Given the results above, discuss the relative computational efficiency of these methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UGNtcOgsQgZ"
      },
      "source": [
        "### Study part: Generalized Policy Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l89FvxifsVib"
      },
      "source": [
        "Code the **generalized policy iteration** algorithm and study the influence of the number of evaluation steps between each improvement step"
      ]
    }
  ]
}